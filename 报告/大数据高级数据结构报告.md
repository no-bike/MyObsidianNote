2022112479 曲本磊

# 实验一 BloomFilter
##  实验1.1

==在1万个正整数中，是否存在某个数。分别使用位图和bloomfilter实现。==

首先使用java编写了一个bitmap，使用int类型存储（发现用long好像也可以）

其中有add、contains、remove三个方法。
```java
public class BitIntmap {  
  
    private final int[] bitmap;  
  
    public BitIntmap() {  
        this.bitmap = new int[93750000];  
    }  
  
    public BitIntmap(int size) {  
        this.bitmap = new int[size];  
    }  
  
    public void add (int value) {  
        int r = value / 32;  
        int c = value % 32;  
        //将第c+1位设为1  
        this.bitmap[r] |= (1 << c);  
    }  
  
    public boolean contains (int value) {  
        int r = value / 32;  
        int c = value % 32;  
        return (this.bitmap[r] & (1 << c)) != 0;  
    }  
  
    public void remove (int value) {  
        int r = value / 32;  
        int c = value % 32;  
        this.bitmap[r] &= ~(1 << c);  
    }  
  
}
```

由于BloomFilter的哈希映射我不会写最后还是选择添加google的guava包，但由于这学期刚接触java对maven依赖的加载不太熟悉，添加依赖以后发现引入的包不是很全，瞎鼓捣三四十分钟以后又莫名其妙好了。

依赖如下：
```xml
<dependency>  
    <groupId>com.google.guava</groupId>  
    <artifactId>guava</artifactId>  
    <version>33.3.1-jre</version>  
</dependency>
```

BloomFilter所需要的三个参数：
1. funnel - the funnel of T's that the constructed BloomFilter will use
2. expectedInsertions - the number of expected insertions to the constructed BloomFilter; must be positive
3. fpp - the desired false positive probability (must be positive and less than 1.0)

本实验的BloomFilter初始化：
```java
BloomFilter<Integer> bloomFilter = BloomFilter.create(  
        Funnels.integerFunnel(),  
        EXCEPTSIZE,  
        FALSERATE  //值为0.001
);
```

分别使用Bitmap和BloomFilter查询，检验错误率：

```java
for (int i = 0; i < 1000; i++) {  
    int num = rand.nextInt(RAND_MAX);  
    if(bloomFilter.mightContain(num) ^ bitIntmap.contains(num)) {  
        wrongnum++;  
    }  
}
```

结果如图：

![[1.png]]

## 实验1.2

因为java对字符串处理比较方便所以把这个也做了。后续文件操作基本都是照着这个改的。

文件操作：
```java
private static Set<String> readUrlsFromFile(String Filename) throws IOException {  
    Set<String> urls = new HashSet<String>();  
    BufferedReader br = new BufferedReader(new FileReader(Filename));  
  
    String line ;  
    while ((line= br.readLine()) != null) {  
        urls.add(line);  
    }  
    br.close();  
    return urls;  
}
```

BloomFilter初始化：
```java
Set<String> urlsFile2 = readUrlsFromFile(File2Name);  
Set<String> urlsFile1 = readUrlsFromFile(File1Name);  
  
BloomFilter<String> bloomFilter = BloomFilter.create(  
        Funnels.stringFunnel(StandardCharsets.UTF_8),  
        urlsFile2.size(),  
        falsePositiveRate  
);
```

将文件二的内容全部put进入后用文件一内容逐个查找，结果保存在urlsFound中，输出如下(下面还有很大一串不截图了)：

![[2.png]]

打开文件一发现它只有30048行，查询成功结果却有30047个，文件一几乎是文件二的子集？

# 实验二 Trie字典树

基于两个实验的要求实现了如下的字典树结构，有的方法没写因为是按需写的方法：

```java
import java.util.ArrayList;  
import java.util.List;  
import java.util.TreeMap;  
  
public class Trie {  
    //树节点  
    class Node{  
        //当前节点字符  
        public Character name;  
  
        //子节点列表  
        public TreeMap<Character, Node> children;  
  
        //是否是单词结尾？  
        public boolean isWordEnd;  
  
        //前缀经过该节点的字符数量  
        public int prefixCount;  
  
        //单词词频  
        public int wordCount;  
  
        //父节点  
        private Node parent;  
  
        public Node(boolean isWordEnd){  
            this(null,isWordEnd,0);  
        }  
  
        public Node(Character name, boolean isWordEnd, int prefixCount) {  
            this.name = name;  
            this.isWordEnd = isWordEnd;  
            this.prefixCount = prefixCount;  
            this.children = new TreeMap<>();  
            this.wordCount = 0;  
        }  
  
        public Node(Character name, boolean isWordEnd, int prefixCount, Node parent) {  
            this(name,isWordEnd,prefixCount);  
            this.parent = parent;  
        }  
  
    }  
  
    private Node root;//根节点  
    //字典树中的单词数目  
    private int size;  
  
    public Trie() {  
        root = new Node(false);  
        size = 0;  
    }  
  
    //插入单词  
    public void insert(String word) {  
        Node curr = this.root;  
        for(char key: word.toCharArray()){  
            if(curr.children.containsKey(key)){  
                curr.children.get(key).prefixCount++;  
            }  
            else{  
                curr.children.put(key, new Node(key,false,1,curr));  
            }  
            curr = curr.children.get(key);  
        }  
        //处理完这个单词后如果它不是结尾则修改。  
        if(!curr.isWordEnd){  
            curr.isWordEnd = true;  
            this.size++;  
        }  
        curr.wordCount++;  
    }  
  
    //删除单词,额暂时不想写  
    public void delete(String word) {  
  
    }  
  
    public int prefixCount(String word) {  
        Node node = getPrefixLastNode(word);  
        return node == null ? 0 : node.prefixCount;  
    }  
  
    public boolean contains(String word) {  
        Node node = getPrefixLastNode(word);  
        return node != null && node.isWordEnd;  
    }  
  
    public int wordCount(String word) {  
        Node node = getPrefixLastNode(word);  
        return node == null ? 0 : node.wordCount;  
    }  
  
    /**  
     * 查找前缀为prefix的所有单词  
     */  
    public List<String> searchPrefix(String prefix) {  
        Node cur = getPrefixLastNode(prefix);  
  
        // 从这个节点往下深搜  
        List<String> paths = new ArrayList<>();  
        dfsSearchAllPath(cur,paths,prefix);  
        return paths;  
    }  
  
    /**  
     * 从节点开始深搜每条路径  
     * @param node              起始节点  
     * @param paths             保存结果的路径  
     * @param curPath           当前搜索的路径  
     */  
    private void dfsSearchAllPath(Node node, List<String> paths, String curPath) {  
        if (node == null || node.children.isEmpty()) {  
            paths.add(curPath);  
            return;  
        }  
  
        for (Node child : node.children.values()) {  
            dfsSearchAllPath(child,paths,curPath + child.name);  
        }  
    }  
  
    // 获取前缀表示的最后一个节点  
    private Node getPrefixLastNode(String prefix){  
        // 往下搜每个字符节点，能搜到结尾即代表存在并返回  
        Node cur = root;  
        for (char key : prefix.toCharArray()) {  
            if(!cur.children.containsKey(key))  
                return null;  
            cur = cur.children.get(key);  
        }  
        return cur;  
    }
```

## 实验2.1

根据输入的文件wordlist1构建trie树，实现统计一个任意输入的单词是否在文件中出现过。

该功能比较简单，只需要调用上述类中的contains方法即可


```java
while (true){  
    System.out.print("\nEnter word you want to search: ");  
    String input = scanner.nextLine();  
  
    if(input.equalsIgnoreCase("exit")){  
        break;  //输入exit会直接跳出，写报告的时候感觉这里其实可以再加一个contains判断exit                 //在不在里面，但是懒得改了。。
    }  
    System.out.print(trie.contains(input));  
}
```

字典树的构造是根据文件按行插入，文件操作也与上个实验类似不展示了。

结果如下：
![[3.png]]

## 实验2.2

之前的文件操作是用集合存储单词，写这部分的时候没注意直接拿来改了改用第一遍出来发现词频全是1，有点搞笑，修改代码如下：

```java
private static List<String> readWorldsFromFile(String Filename) throws IOException {  
    List<String> urls = new ArrayList<>();  
    BufferedReader br = new BufferedReader(new FileReader(Filename));  
  
    String line ;  
    while ((line= br.readLine()) != null) {  
        String[] words = line.split("\\s+");//使用正则表达式匹配一个到多个空格  
        urls.addAll(Arrays.asList(words));  
    }  
    br.close();  
    return urls;  //局部变量名懒得重构。
}
```

wordCount方法写在字典树类里了，循环调用一下即可。

结果（部分）：
![[4.png]]


# 实验三 MapReduce的使用

在虚拟机中Hadoop伪分布式环境搭建过程略，环境启动后进程如下：
![[Pasted image 20241217174700.png]]

依赖如下：
```xml
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.4</version>
    </dependency>
```

代码如下：
```java
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class wordCount {

    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Mapper.Context context) throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {

        String input = "hdfs://localhost:9000/WordCount/input/1.txt"; //文件输入路径
        String output = "hdfs://localhost:9000/WordCount/output";//输出路径
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(wordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(input));
        FileOutputFormat.setOutputPath(job, new Path(output));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}

```

在hdfs存储的结果如下：（部分结果，1.txt，另外三个不再赘述）
![[Pasted image 20241217181036.png]]


# 创新实验
==在线购物网站商品评价推荐系统==

环境：ubuntu22.04虚拟机，伪分布式Hadoop，ideaIDE

用户评价数据集json格式如下：
![[Pasted image 20241217191242.png]]


**因为数据集内容过于庞大，我仅下载了Application类的评价数据集**

其中asin代表产品的具体ID，具体到产品颜色和种类，而parent_asin则仅具体到某种产品，因此我们先对数据进行**降维**，仅保留 ==parent_id==和==rating==两个属性。

使用mapreduce进行降维处理：
```java
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

public class DimensionalityReduction {

    public static class JsonMapper extends Mapper<Object, Text, Text, Text> {
        private ObjectMapper objectMapper = new ObjectMapper();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            JsonNode rootNode = objectMapper.readTree(value.toString());
            JsonNode ratingNode = rootNode.get("rating");
            JsonNode asinNode = rootNode.get("parent_asin");

            if (ratingNode != null && asinNode != null) {
                String rating = ratingNode.asText();
                String asin = asinNode.asText();
                context.write(new Text(parent_asin), new Text(rating));
            }
        }
    }

    public static class JsonReducer extends Reducer<Text, Text, Text, Text> {
        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            for (Text val : values) {
                context.write(key, val);
            }
        }
    }

    public static void main(String[] args) throws Exception {
	    String input = "hdfs://localhost:9000/input/某文件"; //文件输入路径
        String output = "hdfs://localhost:9000/ProductsRecommend/middleReduce";//输出路径
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "dimensionality reduction");
        job.setJarByClass(DimensionalityReduction.class);
        job.setMapperClass(JsonMapper.class);
        job.setCombinerClass(JsonReducer.class);
        job.setReducerClass(JsonReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        FileInputFormat.addInputPath(job, new Path(input));
        FileOutputFormat.setOutputPath(job, new Path(output));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```